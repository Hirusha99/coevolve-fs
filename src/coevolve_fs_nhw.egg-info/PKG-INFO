Metadata-Version: 2.4
Name: coevolve-fs-nhw
Version: 2025.1.0
Summary: A Cooperative Co-evolutionary Genetic Algorithm for feature selection.
Author-email: Hirusha Wanigasingha <hirushanethni2277@gmail.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/Hirusha99/coevolve-fs
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: scikit-learn
Dynamic: license-file

# How to Use coevolve-fs
The library is designed to work seamlessly with Scikit-Learn. It follows a "fit" pattern similar to standard machine learning models.
1. Installation
If you have the library saved locally, install it in your environment using:
bash

```bash
pip install .
```
Use code with caution.

3. Basic Implementation
This example demonstrates how to use the CoevolutionarySelector with a Naive Bayes classifier on a sample dataset.

```python
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from coevolve_fs import CoevolutionarySelector
```

## 1. Prepare your data
```python
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```
## 2. Initialize the Selector
## You can pass any scikit-learn classifier here
```python
classifier = GaussianNB()
selector = CoevolutionarySelector(
    classifier=classifier,
    population_size=50, 
    generations=20,
    crossover_prob=0.8,
    mutation_prob=0.02
)
```
## 3. Run the Co-evolutionary Search
## 'columns_per_subgroup' defines how the feature space is divided

```python
best_mask = selector.fit(
    X_train, y_train, 
    X_test, y_test, 
    columns_per_subgroup=10
)
```

## 4. Apply the results
```python
selected_features = X_train.columns[best_mask == 1]
print(f"Original feature count: {X_train.shape[1]}")
print(f"Reduced feature count: {len(selected_features)}")
print(f"Selected Features: {list(selected_features)}")
```

## 5. Final Model Evaluation
```python
final_model = GaussianNB()
final_model.fit(X_train[selected_features], y_train)
score = final_model.score(X_test[selected_features], y_test)
print(f"Final Accuracy with Selected Features: {score:.4f}")
```
Use code with caution.

### Explanation of Parameters
classifier: Any object with .fit() and .predict() methods (e.g., RandomForest, SVM, or XGBoost).

population_size: The number of chromosomes in each subgroup's population. Higher values explore more but are slower.

generations: How many iterations the evolution process runs for each subgroup.

columns_per_subgroup: The core "Co-evolutionary" parameter. It splits your total features into smaller chunks. For example, if you have 100 features and set this to 10, the library will evolve 10 independent subgroups.

mutation_prob: The chance that a feature's "on/off" status will flip during evolution (helps avoid getting stuck in local optima).

Advanced: Integrating with Pipelines
Because it returns a boolean mask (1s and 0s), you can easily integrate this library into a Scikit-Learn Pipeline by using a FunctionTransformer to filter columns based on the best_mask output.
